2024–2025 医学音频领域心音识别与分类研究进展综述

引言

心音（phonocardiogram, PCG）分析是心血管疾病早期筛查的重要手段之一。通过无创的听诊获取心脏泵血过程中的声音振动，可检测心脏瓣膜病变、结构异常或心功能不全引起的杂音等病理信号 ￼。传统上，心音判读依赖医生经验，准确率随操作者水平差异很大 ￼。近年来，人工智能特别是深度学习技术被广泛应用于心音自动分析，实现对心脏杂音和异常心音的检测 ￼ ￼。本综述聚焦2024年前后该领域的新进展，围绕模型架构、公开数据集、多模态融合、模型性能和临床应用等方面进行综述，并展望未来挑战与趋势。

主流与新兴模型架构

**传统机器学习与CNN/RNN：早期研究多采用手工特征+机器学习方法，如MFCC特征+SVM、HMM等，对PCG进行分类 ￼ ￼。随深度学习兴起，卷积神经网络（CNN）和循环神经网络（RNN）成为主流，用于心音异常检测和心音分段 ￼ ￼。据统计，约68%的相关论文使用CNN架构，33%使用RNN架构 ￼。CNN常用于从时频图（如Mel谱、连续小波变换谱等）提取空间特征，RNN/LSTM用于捕捉心音随时间的周期模式。一些研究在小型数据集上报告了接近100%**的分类准确率（如CNN或自动编码器模型） ￼；但由于评估方式不统一，这些结果的可比性存疑，需要在统一基准下验证其有效性 ￼。总体而言，深度CNN/RNN相较传统方法展现了更强的特征学习与分类性能。例如，Bao 等比较多种模型用于PhysioNet 2016心音数据，发现双向GRU模型取得最高准确率94.07%，灵敏度91.29%，总体评分93.05%（Modified Accuracy，MAcc） ￼ ￼。这表明适当设计的深度模型已可在标准数据集上达到约90%的检测性能。

Transformer 与自监督预训练：近年Transformer架构及自监督音频模型崛起，为心音分析提供了新的Baseline。Vision Transformer (ViT) 和卷积Transformer融合模型已用于心音分类 ￼。例如 Abbas 等构建卷积视觉Transformer（CVT-Trans）模型，将PCG小波谱输入Transformer实现心脏杂音五分类（正常、主动脉狭窄AS、二尖瓣返流MR、二尖瓣狭窄MS、二尖瓣脱垂MVP），在其数据集上准确率达到98%以上 ￼ ￼。Transformer自注意力机制有助于捕捉心音长程依赖和周期结构，一些研究表明Transformer架构可与CNN结合提升分类性能 ￼。除了专门设计的Transformer，通用音频预训练模型正成为新兴基线。例如，wav2vec 2.0是Facebook提出的自监督语音模型，Panah等在2023年将其微调用于心音杂音检测，在PhysioNet/CirCor数据上取得加权准确率0.80、平均召回0.70的性能，与有监督SOTA接近 ￼ ￼。进一步的实验显示，wav2vec模型对标注数据量不敏感，用较小微调集也能保持鲁棒性能 ￼。除了wav2vec，研究者还评估了Audio Spectrogram Transformer (AST) ￼、BYOL-A、BEATs等通用音频基础模型在心音任务的表现，并与微调的专用模型对比 ￼ ￼。Niizumi等的研究表明，不经微调的预训练模型提取的特征在干净的心音数据集上可达SOTA水平（如M2D模型在一清洁数据集上准确率>93%），但在噪声较大的数据上性能仍有差距，需要结合微调优化 ￼ ￼。总体而言，自监督预训练为心音识别提供了新的范式：利用海量非标注心音或广义音频数据学习特征，再通过少量标注数据微调，可缓解医学数据不足的问题 ￼ ￼。未来还需探索更适合心音的预训练策略（目前尚无公开的心音领域大模型 ￼ ￼），例如AudioMAE等方法在PCG上的适用性等，以进一步提升无标注数据的利用效率。

多模态与融合模型：用于临床诊断的心音模型正逐步从单模态音频走向多模态融合，以利用更多患者信息提高判别准确性。一方面是在音频 + 人口统计特征的融合。PhysioNet挑战2022的数据集就提供了每位患者的基本信息（年龄、性别等）和听诊位置等元数据 ￼。顶尖团队通常将这些人口统计特征与音频特征拼接输入模型，从而提高杂音检测性能。例如，有研究针对儿童杂音检测，将年龄作为模型输入一并考虑，因为生理杂音和某些先天性疾病的心音表现与年龄密切相关 ￼ ￼。融合人口统计信息可有效降低漏诊率，被认为是2022年挑战成功方案的关键因素之一。一些方法还结合医师经验规则，如不同听诊位置的杂音阈值，或根据患者体位/病史调整模型阈值，以提升临床适用性 ￼ ￼。另一方面是多传感器信号融合，典型如心音+心电（PCG+ECG）的结合 ￼ ￼。Hangaragi等提出同时利用同步心电与心音信号进行心脏疾病多分类，通过深度融合网络将二者互补信息结合，取得了97%的分类准确率，明显优于仅用单一模态的模型 ￼ ￼。他们指出，仅靠ECG难以捕捉瓣膜杂音，而仅靠PCG又无法发现纯电活动异常，如房颤等 ￼ ￼，因此融合后可覆盖更广病种，实现对主动脉瓣狭窄、二尖瓣病变、房颤、心肌缺血等多类型心疾的有效识别 ￼ ￼。除了ECG，一些研究探索将PCG与颈动脉脉搏波、电子病历文本等信息结合。例如，有概念性研究建议将临床文本记录（如医师的听诊描述、既往病史）输入与音频特征融合，以提供疾病背景知识，从而改进模型判别的特异性。然而，由于缺乏公开的“音频+文本”配对数据，目前此方向报道较少，主要停留在可行性讨论阶段。总体来看，多模态融合在心音智能诊断中展现出巨大发展潜力：融合人口统计与多源生理信号有望显著提高模型稳健性和临床适用性，是未来研究的重点之一。

常用开源数据集概况

高质量的公开数据集对心音算法的发展至关重要 ￼ ￼。目前领域内有若干常用的心音数据集和评测基准：
	•	PhysioNet/CinC Challenge 2016 心音数据库：由刘成玉等整理的开源心音数据库 ￼汇集了来自全球多家研究的九个子数据集，总计包含2435条心音记录，覆盖1297名受试者 ￼。这些记录来自临床及非临床环境，设备多样，长度从数秒到数分钟不等 ￼。每条记录标注为正常或异常心音（部分含不确定标签），目标任务是判别受试者是否需要转诊 ￼ ￼。2016年挑战的评分使用特定代价函数（Cost）的加权准确率：对分类错误赋予不同权重，以平衡漏诊和误报的代价。简单来说，该指标对检测出异常（杂音）给予更高权重，对将不确定案例判断为异常也给予适当奖励 ￼。据报道，2016年比赛优秀算法在测试集上的加权准确率约为0.86–0.89左右，敏感度和特异度约90%上下 ￼ ￼。例如，有方法基于完整经验模态分解+SVM得到87%的准确率 ￼；深度学习方法如GRU可将MAcc提高到93% ￼。该数据集代表了多源异构心音数据融合的早期努力，为之后心音研究提供了宝贵基线 ￼ ￼。其公开发行的论文详细描述了各子数据集的人口统计（年龄、性别）、采样率、传感器类型，以及常用心音分段和分类算法，并提供了Matlab开源基线代码 ￼ ￼以促进后续研究的可比性。
	•	CirCor DigiScope 数据集（PhysioNet Challenge 2022）：这是目前规模最大的公开儿科心音数据库 ￼ ￼。由Oliveira等发布于2022年 ￼并用于当年PhysioNet挑战 ￼。数据收集自巴西的儿童心脏筛查项目，共包含5272条记录，来自1568名0–21岁受试者（平均年龄6.1岁） ￼。每位儿童通常在胸部四个标准听诊位各采集一段心音（包括主动脉瓣区、肺动脉瓣区、三尖瓣区、二尖瓣区），记录长约5–30秒 ￼ ￼。特别地，该数据详细注释了每个记录中心杂音的存在与否，以及杂音的时间位置、形状、音调、高度（Levine分级）、质地和听诊部位等 ￼ ￼。此外还提供了每条记录的S1、S2位置（由3种算法投票标注并经专家修正） ￼。2022年挑战定义了两个任务：(1) 杂音检测：判断每位患者是否存在心杂音（标签为“存在 (present)”、“不存在 (absent)”或“不确定 (unknown)”三类） ￼；(2) 临床结果预测：根据心音推断患者的整体心脏功能是否异常，需要进一步检查治疗。评分方面，Task 1采用上述加权准确率 (W.acc) 作为主要指标，对“杂音存在”和“未知”类别的识别给予更大权重 ￼。Task 2则采用一个成本函数分数衡量临床决策的优劣 ￼ ￼。挑战提供了官方开源Baseline代码和评测脚本，要求参赛队提交可运行代码以确保结果可复现 ￼。该数据集难度较高：真实录音中包含听诊器摩擦、哭闹等各种噪声 ￼ ￼，且杂音类别不平衡，大部分儿童无杂音或杂音轻微（unknown）。在这样的数据上，传统或预训练模型性能下降明显，例如固定参数的AudioSet预训练模型最高W.acc仅0.74 ￼；而需要结合微调才能达到约0.80以上的W.acc ￼ ￼。目前公开报道的最好成绩在该数据集上加权准确率约0.83（由融合多模型微调实现） ￼；有团队基于wav2vec 2.0微调亦达到W.acc 0.80，UAR 0.70，与挑战优胜模型相当 ￼。临床结果预测任务则更具挑战，一般采用与Task1分类结果和患者年龄等特征的联合决策。Manshadi等提出的半监督Stockwell时频特征+AlexNet模型，在Task1上获得平均准确率93%、F1值91% ￼；在Task2上取得5290分的成本函数分数，超越挑战领先方法设定的基线 ￼ ￼。这表明通过专门设计的特征提取和模型组合，可在复杂任务上取得比传统方法更高的临床判别能力 ￼。
	•	PASCAL心音库（Peter Bentley 数据集，2011）：由Bentley等人在2011年PASCAL心音挑战中提供，是较早的公开心音数据集之一 ￼。该库分为数据集A和数据集B两部分：A集来自大众通过手机应用采集的心音，共176段，其中训练集124段包含4类标签（正常、杂音、额外心音、噪音Artifact） ￼；B集来自医院临床的数字听诊器数据，共656段，其中训练集461段为3类标签（正常、杂音、额外收缩） ￼。两部分均提供了部分记录的S1/S2标注及对应的分段任务，挑战要求对未标注测试集进行心音分割和分类 ￼ ￼。该数据集特点是音频数量较少但噪声类型丰富（特别是A集中大量日常环境噪声），分类任务细粒度（最多4类）。由于规模有限，很多后续研究将其用于验证模型对小数据集的学习能力。PASCAL库的A部分常被用于训练或测试四分类模型，如文献报道的多种特征融合+分类器在该数据上的准确率可达90-95% ￼ ￼；也有深度学习方法在A集上实现接近99%的准确率 ￼。然而这些极高性能多数基于交叉验证，在独立测试上效果会下降。总体来说，Peter Bentley心音库为心音分类提供了一个多类别基准，其数据A已被Kaggle等平台收录供教学用途 ￼。需要注意的是，由于数据量很小，现代模型往往轻松过拟合，需配合其他数据集共同评估模型性能。
	•	PASS心音数据集：PASS是近年来提及的另一个公开心音库，包含数量较少但标注精细的心音记录。据报道，PASS数据集中总共收录了79条PCG记录，其中39条为正常心音，40条为病理性心音 ￼。正常心音部分又根据采集状态细分成不同子类，以提供更丰富的对照。该数据集由研究者公开用于模型评估，旨在弥补大型数据集中某些细粒度类别不足的情况。由于规模有限，PASS常用于与其他数据集结合训练或验证模型泛化能力。例如，在一项心音分类研究中，作者将PASS数据用于测试SVM和KNN模型的识别能力，结果分别 đạt到了**98-99%**的准确率 ￼（可能因测试集很小）。因此，PASS更多地充当辅助验证集或数据增强来源，而非独立训练集。在学术报告中对其提及较少，但在一些综述性文章中被列为可用的开放数据之一。鉴于信息有限，PASS具体涵盖的心脏病种类、采集背景等公开资料较少，需要进一步权威来源加以说明。
	•	其他数据集：除了上述主流数据库，还有一些近年发布或常用的心音数据集值得一提。GitHub开放心音数据集：Yaseen等人在2018年整理了一套公开PCG数据（简称“开放心音数据集”），包含1000条心音音频，标注为正常以及四类常见杂音（AS、MR、MS、MVP）各200例 ￼。由于数据无底噪干扰且均衡，许多算法在该数据上取得极高精度，如KNN分类准确率达99.5% ￼。BUET多疾病心音数据集（BMD-HS，2023）：由孟加拉国工程大学发布，包含864段心音，分属5类心脏瓣膜病（主动脉狭窄等），并引入多标签标注以表示杂音严重程度和合并症情况 ￼ ￼。该数据集专注于复杂瓣膜病案例，提供了挑战性的多标签分类任务。目前BUET数据用于验证预训练模型在更干净数据上的有效性：冻结预训练模型的特征在BMD-HS上可取得>93%的准确率，明显优于在噪声数据集上的表现 ￼ ￼。此外，还有Michigan心音库、Shenzhen心音数据集(HSS)等提供特定人群（如某医院患者或特定地域）的心音记录 ￼ ￼。这些数据集各有侧重，例如包含详细的临床诊断信息或同步采集的其他信号（如颈动脉搏动、血压等 ￼），为多模态研究提供了可能。综合来看，目前心音研究常用的数据集涵盖了成人与儿童、正常与多种异常、受控环境与真实环境等多种情境。研究者应根据任务需求选择合适的数据集，并关注不同数据集之间标注标准的不一致性对模型评估的影响 ￼ ￼。

模型性能与基线对比

不同模型在上述数据集上的性能表现各异，需要在统一指标下比较。总体而言，分类准确率、敏感性、特异性和F1值是常用指标，而一些挑战赛采用特别的加权准确率或成本函数作为评判标准 ￼ ￼。以下结合代表性数据集，概述主流模型的对比性能：
	•	在PhysioNet 2016数据集上的性能：该任务为二分类（正常/异常）。传统机器学习方法（如SVM、随机森林）结合经验特征通常能达到80%-85%的准确率 ￼ ￼。挑战优胜算法多采用集成方法：例如划分心音周期段提取多维特征送入梯度提升等模型，可获得约90%的交叉验证准确率 ￼ ￼。深度学习方法逐渐展现优势。Yang等在2016数据上用改进的Mel频率特征+ResNet模型达到95%的准确率 ￼ ￼。另一项研究中，CNN+LSTM端到端模型获得约88%的准确率和F1分数 ￼ ￼。需要注意，不同论文常使用不同的训练/测试拆分，直接比较绝对数值并不严谨 ￼。PhysioNet官方报告中，最佳算法在隐藏测试集上的Challenge评分为0.86（以加权准确率衡量） ￼。因此可以认为，在2016数据上经过优化的模型可以实现**85-90%**左右的实际识别性能。一些论文报告接近100%的准确率往往是在小规模验证下取得，缺乏统计显著性 ￼。总体趋势是：深度模型性能≈高级机器学习方法，但深度模型具有端到端学习和跨数据集迁移的潜力，这为后续改进奠定基础。
	•	在CirCor/PhysioNet 2022数据集上的性能：该任务为三分类（杂音有/无/未知）且数据噪声大、类别不平衡，难度较高。官方Baseline采用简单的机器学习方案，成绩较低；而参赛团队通过深度学习和融合策略显著提升了性能 ￼ ￼。据统计，参赛的87个算法中许多采用CNN或RNN架构，也有Transformer和基于迁移学习的方法 ￼ ￼。最终排名前列的方法加权准确率（W.acc）约在0.79~0.83之间 ￼。例如，剑桥大学团队采用半马尔可夫模型(HSMM)结合深度特征，取得W.acc≈0.80 ￼；Jena大学团队用深度集成模型（CNN+XGBoost融合）也达到约0.80 ￼。NTT实验室研究显示，通过微调AudioSet预训练模型(M2D)可达0.832的W.acc，超过传统方法性能 ￼。自监督wav2vec微调模型则获得0.80 W.acc，与上述方法相差不大 ￼。未微调的预训练模型表现稍逊（最高M2D冻结特征0.737 W.acc） ￼。各模型在敏感性（检出杂音）与特异性（识别无杂音）上存在差异：有的模型能高检出率识别杂音present但容易将无杂音误判为未知 ￼ ￼。例如，BEATs预训练模型倾向于预测“未知”，导致present召回率很低 ￼ ￼。综合来看，在2022高噪声数据上，目前最佳方法的加权准确率尚未超过0.85，仍有提升空间。此外，对Task 2临床结果的预测，各队普遍采用在Task1基础上增加一个分类器或回归模型。一些顶尖队伍在Task2上的成本分数约5000左右，而Manshadi等方法达到5290，表明其在综合判断正常/异常心功能上超过其他方法 ￼。这可能归功于该方法有效融合了时频深度特征和传统分类器，使模型在数据有限情况下表现更鲁棒 ￼ ￼。
	•	多数据集综合对比：为了全面评估模型，一些研究在多套数据集上测试。例如Abbas等综述了五分类心音（正常+4种杂音）任务的多篇结果：传统方法准确率在93-95% ￼；PhysioNet 2016和PASCAL数据上的梯度提升法约90.25%准确率 ￼；Mel谱CNN模型在GitHub五分类数据上可达98-99%准确率 ￼。这些结果表明，在受控环境、小规模数据下，模型可以非常高的拟合度完成任务，但在更复杂、更真实的数据上，性能会明显下降。Niizumi等的跨数据研究提供了有益视角：基础模型在干净的BUET数据（五类瓣膜病）上几乎都超越了传统微调模型（最高准确率95.2%，F1约0.88） ￼ ￼；但在嘈杂的CirCor数据（三分类杂音）上，无论微调与否都难以逼近0.8以上W.acc ￼。这强调了数据集质量和分布对模型性能的影响巨大。因此，报告模型性能时应注明所用数据集和评测方式，避免片面比较绝对数值 ￼。总体而言，目前公开数据上的心音分类准确率大致范围为：在理想条件下（二分类、小数据集）可超过95%；在中等难度条件下（二分类大数据或多分类小数据）约85-95%；在挑战性条件下（三分类噪声数据）约75-85%（加权准确率)。今后的目标是通过更健壮的模型与训练策略，把复杂场景下的性能提高到90%以上水平，从而满足临床实用需求。

公开基线系统与工具包：在心音研究社区，公开的基线代码和工具有助于推动算法改进。PhysioNet挑战系列为2016和2022年的比赛都发布了开源基线实现：例如2016年提供了Matlab范例代码（基于Springer HSMM算法进行心音分段+分类） ￼；2022年则要求所有提交均提供可运行代码，并汇总了参赛论文和源码链接 ￼ ￼。这些资源让研究者可以复现实验结果并与挑战Top算法对比。此外，一些通用的听诊音分析工具包也在使用中。例如，MIT开源的HeartPy和波形处理库虽然主要面向脉搏/心率提取，但也可用于心音信号预处理。深度学习方面，许多研究使用PyTorch或TensorFlow实现模型，并借助LibROSA等音频处理库提取特征。值得一提的是，近年涌现的HuggingFace音频模型库收录了AST、wav2vec2.0、BEATs等预训练模型，使研究者能方便地加载这些基础模型用于心音任务 ￼ ￼。例如，通过transformers库加载wav2vec2模型，接上若干全连接层微调，即可复现Panah等人的杂音检测结果。还有一些团队构建了心音专用的开源框架：如上海大学开发的心音分类平台，整合了数据预处理、分段、特征提取和多模型训练流程，提供API方便地比较不同算法性能。据报道，使用该平台的ExtraTrees分类器在一个五分类心音数据集上达到95.7%准确率，展示了工具化的威力 ￼ ￼。总的来说，随着社区共建，心音分析正逐步形成标准化评测流程和开源代码库。公开Baseline系统不仅提升了研究的透明度和可重复性，也降低了新算法入门门槛。例如，2022挑战涌现的大量创新方法，很大程度上得益于主办方提供的数据和基线以及参赛者开放分享的模型代码 ￼ ￼。未来，期待有更加完善的一站式心音AI工具包出现，涵盖从信号获取、降噪、分段、特征到分类诊断的完整流程，加速科研到临床应用的转化。

临床部署、泛化与未来趋势

尽管心音智能分析取得显著进展，但距离实际临床部署仍有诸多挑战需要克服。

模型泛化能力：当前模型往往对训练数据的依赖较强，在分布偏离的测试数据上性能显著下降 ￼ ￼。造成这一问题的原因包括：不同设备采集的心音差异、环境噪声干扰、患者人群差异（成人 vs 儿童）等。例如，在噪声水平更高的CirCor数据上，同一模型的表现远逊于安静条件下的数据 ￼ ￼。再如，某些模型在训练数据中学到了心音长度或响度的规律，但在现实中遇到异常更短或更微弱的杂音可能会漏检。为提升泛化，研究者正探索数据增强和域适应技术，如加入随机背景噪声、频谱失真、心率变化等来扩充训练数据的多样性 ￼。有学者提出PCGmix等专门的数据增强方法，将不同心音信号混合以模拟噪声条件，结果有效提高了模型稳健性 ￼。另外，迁移学习和跨域微调也被用于心音领域：Costa等在2022挑战中应用多任务迁移学习，将正常/异常分类与杂音分级等任务一起学习，以利用相关任务信息提升主要任务表现 ￼ ￼；Li等尝试将其他音频任务上预训练的模型迁移到心音分类，通过少量心音数据微调，取得比从零训练更高的准确率 ￼。这些结果表明，充分利用外部数据和任务关联，可以在一定程度上缓解训练数据不足导致的过拟合。但迁移模型也可能带来负迁移（不相关特征干扰），需谨慎选择源任务和层冻结策略。

临床部署考虑：在实际应用中，模型需在资源受限的终端（如电子听诊器或移动设备）上实时运行。这对模型计算复杂度和能耗提出要求。一方面，模型必须足够轻量，如采用更小的卷积核、更浅层数，或通过模型蒸馏压缩Transformer等，以确保在移动CPU上以接近实时的速度处理每条心音（通常<30秒音频）。另一方面，延迟和可靠性也很重要：医生希望在听诊同时获得模型反馈，因此推理延迟最好低于1秒；系统需具有容错性，不能因偶发噪声就崩溃或给出荒谬结果。因此，有研究致力于简化模型架构，如使用一维CNN提取简单纹理特征实现快速检测 ￼ ￼。Hsieh等在2025年的研究中开发了残差-循环混合网络部署于临床，单条记录级别准确率88.5%，在病人级别通过融合多导数据达90%准确率，同时模型保持了较低的计算开销，证明了在真实环境中可行 ￼ ￼。未来，随着硬件进步，也可以考虑将部分模型部署在云端，通过5G等实时将听诊数据上传服务器分析，再将结果返回终端显示，以平衡终端负载与性能。

多模态融合效益：正如前文讨论，多模态数据融合是提升模型实用性的有效途径。在临床中，医生诊断并不只听声音，还会考虑患者的年龄、症状、既往史等。引入结构化数据（年龄、性别、体表指标）和非结构化数据（如医生的听诊结论、心脏彩超报告）将有助于算法更全面地判断。已有研究证明，加入年龄等人口统计变量可提高杂音检测的特异度，减少将年轻健康儿童正常杂音误判为病理的情况 ￼ ￼。同时，结合心电图等信号可以区分因心律失常导致的异常心音与因瓣膜病导致的杂音，减少误诊 ￼ ￼。可以预见，未来的智能听诊器或移动心音分析App，可能会同时采集心音、心电、血氧、血压等多模态生理参数，综合评估心脏状态。这需要开发多模态深度融合模型，在架构上实现不同模态特征的有效交互。目前一些探索性工作（如Hangaragi等）已取得初步成功 ￼ ￼。关键问题在于不同模态的数据同步和标注困难，以及融合后的模型如何保持可解释性（见下）。随着更多多模态数据公开（例如含同步PCG和ECG的EPHNOGRAM数据库等 ￼），这一方向有望快速推进。

模型可解释性与医师信任：在临床环境，算法给出的诊断结果需要令医师信服。黑箱性质的深度模型若无法解释其依据，医生往往难以放心依赖。因此，可解释AI在心音应用中尤为重要。一些研究开始尝试揭示模型决策依据。例如，利用SHAP值分析模型对输入心音各部分的依赖程度，发现模型主要依据的是S1和S2附近的信号差异 ￼；当人为去除S1/S2成分后，模型输出显著变化，印证了模型决策与经典听诊要点一致 ￼。还有研究使用类Grad-CAM的时频热力图标注，直观显示CNN在Mel谱上关注的区域往往对应杂音出现的频带 ￼。这些尝试增强了模型的透明度，让临床医生可以理解“模型认为这段心音有杂音，是因为在收缩期出现了异常频谱能量”之类的依据。有了可解释性，医生可将模型作为辅助工具，与自身听诊经验相互印证，提高诊断信心。同时，可解释性分析也能帮助工程师发现模型错误学习的模式并加以修正。未来，可考虑与心血管领域专家深度合作，设计符合医理的模型结构（例如显式分段S1/S2、分别判断收缩/舒张期），并通过可解释性技术验证模型输出与医学知识的一致性。这将有助于推动监管部门和临床用户接受AI听诊系统。

面临的其他挑战：一是数据获取和标准化。尽管已有多个公开数据集，但心音数据的标注仍耗费专家精力且存在主观差异。未来需要建立更标准的听诊记录和标注规范，可能借助云平台让心脏科医生协作标记大量数据，并采用一致的杂音分级标准，从而生成大规模高质量的训练集。二是特殊病例与罕见疾病的识别。目前模型主要针对常见杂音，而某些罕见先心病的心音特征很少被模型“见过”，容易漏诊。解决方法包括数据增广生成仿真心音，或研发结合生理建模与深度学习的 hybrid 模型，将医学先验知识融入模型以处理零样本病例。三是实时监测与远程医疗。随着可穿戴设备的发展，将来或可持续采集日常心音以检测阵发异常，这要求算法具有极低误报率以避免频繁扰民，同时能在长时序数据中定位短暂异常。针对此，研究者需优化算法的阈值策略，或引入异常检测模块，仅在高置信度时才报警。总之，要让心音AI真正走进临床，必须在准确性、可靠性、可解释性和易用性上同时下功夫。

未来展望：2024–2025年的研究趋势表明，心音智能分析正朝着“更智能、更全面、更可信”的方向发展。大模型时代的来临为心音领域提供了新机遇，或许不久将出现专门针对生理音频（包括心音、肺音等）的基础模型，大规模自监督预训练将进一步释放未标记数据的价值 ￼ ￼。同时，多模态融合会越来越深入，从目前的音频+静态人口学信息，扩展到音频+动态影像（如心超、心音同步的B超视频）等，提供前所未有的诊断维度。最后，在临床实际应用方面，随着法规逐步明确医疗AI的审批要求，心音分析算法应尽早围绕安全性和有效性开展临床试验，积累真实世界证据。一旦证明这些算法可以可靠地辅助医生发现心脏杂音、筛查结构性心脏病，其社会效益将是巨大的——尤其在基层医疗匮乏地区，智能听诊将极大提高先天性心脏病和风湿性心脏病的早期检出率 ￼ ￼。展望未来，心音识别与分类研究将在医学和人工智能的交叉协作下继续蓬勃发展，从而推动实现“人工智能听诊器”这一智慧医疗的美好愿景。

参考文献
	1.	Reyna MA et al. Heart Murmur Detection from Phonocardiogram Recordings: The George B. Moody PhysioNet Challenge 2022 ￼ ￼.
	2.	Oliveira J et al. The CirCor DigiScope Phonocardiogram Dataset ￼ ￼.
	3.	PhysioNet/Computing in Cardiology Challenge 2016 Announcement ￼ ￼.
	4.	Liu C et al. An open access database for the evaluation of heart sound algorithms ￼ ￼.
	5.	Manshadi OD & Mihandoost S. Murmur identification and outcome prediction using Stockwell transform (Sci. Reports 2024) ￼ ￼.
	6.	Niizumi D et al. Assessing Audio Foundation Models for Heart Sound Analysis (arXiv 2025) ￼ ￼.
	7.	Shariat Panah D et al. Exploring wav2vec 2.0 for heart murmur detection (EUSIPCO 2023) ￼ ￼.
	8.	Abbas Q et al. Automatic PCG disorder detection using Conv-ViT (Diagnostics 2022) ￼ ￼.
	9.	Scientific Reports volume 15, article 8129 (2025): ECG and PCG fusion for heart disease classification ￼ ￼.
	10.	Abnormal heart sound recognition using SVM and LSTM (Sci. Reports 2025) ￼ ￼.
	11.	Partovi E et al. Deep learning methods for heart sound analysis (Frontiers AI 2024) ￼ ￼.
	12.	Hangaragi S et al. Integrated Residual-RNN for pediatric murmur detection (Sci. Reports 2025) ￼ ￼.
	13.	Heart Sounds Shenzhen (HSS) Dataset Introduction ￼ ￼.
	14.	Ali SN et al. BUET Multi-disease Heart Sound Dataset (arXiv 2023) ￼ ￼.
	15.	PhysioNet 2016 Challenge Evaluation Script ￼ ￼.
	16.	PhysioNet 2022 Challenge Results (CinC 2022 paper index) ￼ ￼.
	17.	Bondareva E et al. Uncertainty-aware murmur detection via tandem learning (CinC 2022) ￼ ￼.
	18.	Chen T et al. PCG classification with Whisper model (2023) ￼.
	19.	McDonald A et al. Parallel HSMM for murmur detection (CinC 2022) ￼.
	20.	Monteiro S et al. BLSTM for murmur & outcome (CinC 2022) ￼ ￼.
	21.	Bao W et al. Heart sound duration classification with RNNs (2022) ￼ ￼.
	22.	Zhang Y et al. Mel-spectrogram + ResNet for heart sound (2021) ￼ ￼.
	23.	Hu L et al. ResNet for murmur detection (CinC 2022) ￼.
	24.	Bentley P et al. PASCAL Heart Sounds Challenge 2011 ￼ ￼.
	25.	Larsen LH et al. Data augmentation for PCG classification (2023) ￼.